{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1cc689-652f-47d7-9f30-260b0e778b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5532a107-8443-4333-8e37-d98a14167280",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from encodec import EncodecModel\n",
    "from encodec.utils import convert_audio\n",
    "\n",
    "import torch\n",
    "\n",
    "bd = 3.0 #bandwidth\n",
    "cd = int(bd*(2/3)) #codebook dim\n",
    "\n",
    "enc_model = EncodecModel.encodec_model_48khz()\n",
    "enc_model.set_target_bandwidth(bd)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "enc_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531b0103-d2e1-41da-964a-215edbb14cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Model, GPT2Config, Trainer, TrainingArguments\n",
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "from tqdm import tqdm\n",
    "\n",
    "np = 2688 #(Number of Positional embeddings) - max sequence length\n",
    "\n",
    "checkpoint_dir = f\"./results-{np}-{bd}-kbps/checkpoint-915000/\"\n",
    "\n",
    "from IPython.display import Audio\n",
    "\n",
    "def codes_to_wav(codes):\n",
    "    \n",
    "    data = codes.reshape(1, codes.shape[0]//cd, cd).T.squeeze().unsqueeze(0).to(device)\n",
    "    \n",
    "    enc_frames = [(data, None)]\n",
    "        \n",
    "    data = enc_model.decode(enc_frames)\n",
    "    \n",
    "    return data\n",
    "\n",
    "class DanceDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.file_list = os.listdir(data_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.data_dir, self.file_list[idx])\n",
    "        tensor = torch.load(file_path).detach().cpu()\n",
    "        last_dim = tensor.shape[-1]\n",
    "        start_idx = torch.randint(0, max(last_dim - np//cd, 1), (1,)).item()\n",
    "        end_idx = min(start_idx + np//cd, last_dim)\n",
    "        sample = tensor[:, :, start_idx:end_idx]  #Take a random slice of audio from the current audio file\n",
    "\n",
    "        input_ids = sample.mT.flatten()\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        labels = sample.mT.flatten()\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "custom_config = GPT2Config(\n",
    "    vocab_size=1026,\n",
    "    n_positions=np, #max positional embeddings\n",
    "    n_layer=12,\n",
    "    n_embd=768,\n",
    "    n_head=12,\n",
    "    n_inner=3072,\n",
    "    decoder_start_token_id=1024,\n",
    "    pad_token_id=1024,\n",
    "    eos_token_id=1025\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe5887e-1370-465a-8d30-11d08a53fd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "resume_from_checkpoint = True #Set to False for fresh training run\n",
    "\n",
    "#Load trained GPT-2 model with custom configuration\n",
    "if resume_from_checkpoint:\n",
    "    #Load from checkpoint\n",
    "    model = GPT2LMHeadModel.from_pretrained(checkpoint_dir)\n",
    "else:\n",
    "    #Fresh training\n",
    "    model = GPT2LMHeadModel(custom_config)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "dance_dataset = DanceDataset(\"dance-data\\\\encoded-44khz-3kbps\")\n",
    "data_loader = DataLoader(dance_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "for batch in data_loader:\n",
    "    print(\"Batch shape:\", batch[\"input_ids\"].shape)\n",
    "    print(batch[\"input_ids\"])\n",
    "    audio = codes_to_wav(batch[\"input_ids\"].squeeze(0))\n",
    "    break\n",
    "#Audio(data=audio.squeeze().detach().cpu().numpy(), rate=48000, autoplay=True)\n",
    "\n",
    "print(batch[\"input_ids\"].device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c5f690-cf95-4deb-b7bb-9da73e9087ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "def evaluate_fn(step, temperatures=[1.5, 2.0, 2.5, 3.0, 3.5], save_dir=f\"audio_samples-{np}-{bd}-kbps\"):\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    #Generate audio samples\n",
    "    for temperature in tqdm(temperatures, desc=\"Temperature\"):\n",
    "        print(\"test\")\n",
    "        input_ids = torch.tensor([[0]]).to(device)  #start token\n",
    "        output = model.generate(input_ids=input_ids, min_length=np-1, max_length=np, do_sample=True, temperature=temperature, num_return_sequences=1)\n",
    "        output = output.squeeze(0)\n",
    "        audio_sample = codes_to_wav(output)\n",
    "        audio_data = audio_sample.squeeze().detach().cpu().numpy()\n",
    "        file_path = os.path.join(save_dir, f\"audio_sample_step_{step}_temp_{temperature}.wav\")\n",
    "        torchaudio.save(file_path, torch.from_numpy(audio_data), 48000)\n",
    "\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a4cdfe-fcf4-4e04-8032-06d7862b029a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(data=audio.squeeze().detach().cpu().numpy(), rate=48000, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cf3524-4856-4939-93d7-079aa223ed71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "wandb.init(project=\"dance-44khz\")\n",
    "\n",
    "train_size = int(0.95 * len(dance_dataset))\n",
    "eval_size = len(dance_dataset) - train_size\n",
    "train_dataset, eval_dataset = random_split(dance_dataset, [train_size, eval_size])\n",
    "\n",
    "#Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./results-{np}-{bd}-kbps\",\n",
    "    num_train_epochs=1000,\n",
    "    save_total_limit=5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    save_steps=5000,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5000,\n",
    "    report_to=\"wandb\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "#callback will generate new 5 audio samples (at varying temperature settings) every 5000 iterations\n",
    "class CustomCallback(TrainerCallback):\n",
    "    def on_train_begin(self, args, state, control, model=None, tokenizer=None, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        global_step = state.global_step\n",
    "        if global_step % 5000 == 0:\n",
    "            evaluate_fn(global_step)\n",
    "\n",
    "custom_callback = CustomCallback()\n",
    "\n",
    "trainer.add_callback(custom_callback)\n",
    "\n",
    "#Perform initial evaluation\n",
    "#initial_evaluation_result = model.evaluate(eval_dataset=eval_dataset, step=0, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "#Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037ca689-5b11-4dca-9bc8-3758b386669e",
   "metadata": {},
   "source": [
    "### INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7070ce-a03c-444f-9a48-b70e02b08f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "#Load the trained model\n",
    "model_path = checkpoint_dir\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5a1e08-ff64-47c6-ac0e-f618f3344a88",
   "metadata": {},
   "source": [
    "### Generation Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8799e5c0-5758-4c5f-8996-71ddd9e55a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "max_length = np \n",
    "\n",
    "input_ids = torch.tensor([[0]]).to(device)\n",
    "output = model.generate(input_ids=input_ids, max_length=max_length, do_sample=True, temperature=2.5, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8296e668-fa58-4223-ac18-6b32db8d100d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698fd272-6dd8-427b-a57e-996c42a88345",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = output.squeeze(0)\n",
    "audio = codes_to_wav(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4107fc4-6a57-4994-843d-aa4f87683f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "Audio(data=audio.squeeze().detach().cpu().numpy(), rate=48000, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84dc0ca-0585-4d9e-8660-59ee723b13c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6522403-3828-4554-ad2e-b69582538f82",
   "metadata": {},
   "source": [
    "### Musical Continuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc219817-ea2a-4b43-b970-b44e00480687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import torch, torchaudio\n",
    "\n",
    "#Load wav file which you want to extend\n",
    "file_path = 'example_audio//sample.wav'\n",
    "wav, sr = torchaudio.load(file_path)\n",
    "\n",
    "wav = convert_audio(wav, sr, enc_model.sample_rate, enc_model.channels)\n",
    "wav = wav.unsqueeze(0).to(device)\n",
    "\n",
    "#Calculate the start and end index for the audio clip\n",
    "start_time = 7  #Start time (s)\n",
    "end_time = 15  #End time (s)\n",
    "start_index = int(start_time * sr)\n",
    "end_index = int(end_time * sr)\n",
    "\n",
    "segment = wav[:, :, start_index:end_index]\n",
    "\n",
    "segment_tensor = torch.tensor(segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a335d84c-9682-4489-ba71-79a3336ad4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(segment_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3639646-2445-4a13-9f6c-07b0f63d684d",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = segment_tensor.to(device)\n",
    "\n",
    "#Extract codes\n",
    "with torch.no_grad():\n",
    "    encoded_frames = enc_model.encode(audio)\n",
    "codes = torch.cat([encoded[0] for encoded in encoded_frames], dim=-1)  # [B, n_q, T]\n",
    "\n",
    "codes = codes.mT.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916ab931-0100-4996-991e-138cb0b53db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(codes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f8479a-ab60-406e-ba21-2c92455b4392",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "max_length = np \n",
    "\n",
    "all_ids = []\n",
    "\n",
    "input_ids = codes.unsqueeze(0).to(device)\n",
    "    \n",
    "w = int(np//2) #(window) size\n",
    "    \n",
    "for i in tqdm(range(4)):\n",
    "    output = model.generate(input_ids=input_ids, max_length=max_length, do_sample=True, temperature=1.3, num_return_sequences=1)\n",
    "\n",
    "    last_ids = output[:, -(np-w):]\n",
    "    \n",
    "    if i == 0:\n",
    "        all_ids.extend(output.squeeze().tolist())\n",
    "    else:\n",
    "        all_ids.extend(last_ids[:, -w:].squeeze().tolist())\n",
    "    \n",
    "    input_ids = torch.tensor(last_ids).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a6176a-598d-49ba-b719-435b5d46e86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = torch.tensor(all_ids)\n",
    "    \n",
    "output = all_ids.squeeze(0)\n",
    "audio = codes_to_wav(output)\n",
    "\n",
    "gen_save_dir = f\"cont_audio_samples-{np}-{bd}\"\n",
    "\n",
    "os.makedirs(gen_save_dir, exist_ok=True)\n",
    "    \n",
    "audio_data = audio.squeeze().detach().cpu().numpy()\n",
    "file_path = os.path.join(gen_save_dir, f\"audio_sample.wav\")\n",
    "torchaudio.save(file_path, torch.from_numpy(audio_data), 48000)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06331c4d-4064-40ad-ae2e-77ab90e34df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "Audio(data=segment_tensor.squeeze().detach().cpu().numpy(), rate=48000, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c6ea06-3b53-4a47-86e7-40aea95465c3",
   "metadata": {},
   "source": [
    "### Longer Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0c13d4-17cf-46bc-badf-2e857829ba06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torchaudio\n",
    "\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "max_length = np \n",
    "\n",
    "for h in range(200):\n",
    "\n",
    "    all_ids = []\n",
    "    \n",
    "    input_ids = torch.tensor([[0]]).to(device)\n",
    "    \n",
    "    w = int(np//2) #(window) size\n",
    "    \n",
    "    for i in tqdm(range(10)):\n",
    "        output = model.generate(input_ids=input_ids, max_length=max_length, do_sample=True, temperature=1.5, num_return_sequences=1)\n",
    "    \n",
    "        last_ids = output[:, -(np-w):]\n",
    "    \n",
    "        if i == 0:\n",
    "            all_ids.extend(output.squeeze().tolist())\n",
    "        else:\n",
    "            all_ids.extend(last_ids[:, -w:].squeeze().tolist())\n",
    "    \n",
    "        input_ids = torch.tensor(last_ids).to(device)\n",
    "    \n",
    "    all_ids = torch.tensor(all_ids)\n",
    "    \n",
    "    output = all_ids.squeeze(0)\n",
    "    audio = codes_to_wav(output)\n",
    "\n",
    "    gen_save_dir = f\"gen_audio_samples-{np}-{bd}\"\n",
    "\n",
    "    os.makedirs(gen_save_dir, exist_ok=True)\n",
    "    \n",
    "    audio_data = audio.squeeze().detach().cpu().numpy()\n",
    "    file_path = os.path.join(gen_save_dir, f\"audio_sample_{h}.wav\")\n",
    "    torchaudio.save(file_path, torch.from_numpy(audio_data), 48000)\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0106225-2730-4582-9f2c-f11740796a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e72e15-5103-4363-b017-b88e69a71e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "Audio(data=audio.squeeze().detach().cpu().numpy(), rate=48000, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcde296-3665-4893-9b49-08759c3cf988",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e565b1bb-be87-4e99-8514-4fd694df28f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faa2826-6324-4c6d-a989-ce2bb6bb22ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c5c1e5-9713-4c4d-b98e-e7b422595dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_ids[2800:2850])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81e0391-4998-4f72-8c84-3fdea7dec35c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
